CORD-19 Data Explorer

This project is a beginner-friendly data analysis and visualization assignment using the CORD-19 metadata dataset. It demonstrates fundamental data science workflows including data loading, cleaning, exploration, visualization, and deployment with Streamlit.

â¸»

ğŸš€ Project Overview

The project walks through:
	â€¢	Loading and exploring the metadata.csv file from the CORD-19 dataset.
	â€¢	Performing basic data cleaning (handling missing values, date conversion).
	â€¢	Conducting simple data analysis (papers per year, top journals, word frequency).
	â€¢	Creating visualizations with Matplotlib/Seaborn/WordCloud.
	â€¢	Building a simple Streamlit application for interactive exploration.

â¸»

ğŸ“‚ Dataset Information

We use the metadata.csv file from the CORD-19 Research Challenge.
This file contains metadata about research papers including:
	â€¢	Paper titles and abstracts
	â€¢	Publication dates
	â€¢	Authors and journals
	â€¢	Source information

âš ï¸ Note: The full dataset is large. For this assignment, we work with just the metadata file or a smaller sample.

â¸»

ğŸ› ï¸ Tools & Requirements
	â€¢	Python 3.7+
	â€¢	pandas (data manipulation)
	â€¢	matplotlib, seaborn (visualization)
	â€¢	wordcloud (title frequency)
	â€¢	streamlit (web application)
	â€¢	Jupyter Notebook (optional, for exploration)
 Install dependencies:
 pip install pandas matplotlib seaborn streamlit wordcloud
 ğŸ“Œ Tasks Completed

ğŸ”¹ Part 1: Data Loading & Exploration
	â€¢	Loaded metadata.csv into a DataFrame
	â€¢	Checked shape, data types, missing values, and statistics

ğŸ”¹ Part 2: Data Cleaning & Preparation
	â€¢	Removed rows with missing critical values (e.g., publish_time)
	â€¢	Filled missing journals with "Unknown"
	â€¢	Converted publish_time â†’ datetime & extracted year
	â€¢	Created new feature: abstract_word_count

ğŸ”¹ Part 3: Analysis & Visualization
	â€¢	Papers per year (trend over time)
	â€¢	Top 10 journals publishing COVID-19 research
	â€¢	Word frequency cloud of paper titles
	â€¢	Distribution by source

ğŸ”¹ Part 4: Streamlit Application

A simple interactive web app with:
	â€¢	Year range filter (slider)
	â€¢	Sample data viewer
	â€¢	Bar chart of publications by year
	â€¢	Top journals bar chart
 Run the app:
 streamlit run app.pyğŸ“¸ Example Outputs
	â€¢	Publications by year (bar chart)
	â€¢	Top 10 journals (horizontal bar chart)
	â€¢	Word cloud of titles

(Add screenshots here after running your notebook/app)

â¸»

ğŸ“– Reflections
	â€¢	Challenges: Handling missing data and managing the dataset size.
	â€¢	Learnings: Improved confidence in Pandas, basic visualization, and deploying Streamlit apps.
	â€¢	Next steps: Expand analysis with NLP (abstract/topic modeling), add more interactive filters to Streamlit.
 ğŸ“ Repository Structure
 Frameworks_Assignment/
â”‚
â”œâ”€â”€ app.py              # Streamlit app
â”œâ”€â”€ analysis.ipynb      # Jupyter Notebook for exploration
â”œâ”€â”€ metadata.csv        # Dataset (or sample of it)
â”œâ”€â”€ README.md           # Documentation
â””â”€â”€ requirements.txt    # Dependencies
âœ¨ Author

Developed as part of a Data Analysis & Streamlit assignment.
 
